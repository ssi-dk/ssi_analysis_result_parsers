{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directive for creating a script for your notebook\n",
    "\n",
    "The block here below is required at the top of each notebook that you want to create a script for. You will also need to edit the \"settings.ini\" file, to create a script (see [Coding in NBdev](https://dksund.sharepoint.com/:fl:/g/contentstorage/CSP_7c761ee7-b577-4e08-8517-bc82392bf65e/ETlSfUyArSNJhX8veMI_JQ8By1aXGHzDJkhotpfpXx4mmw?e=037EwH&nav=cz0lMkZjb250ZW50c3RvcmFnZSUyRkNTUF83Yzc2MWVlNy1iNTc3LTRlMDgtODUxNy1iYzgyMzkyYmY2NWUmZD1iJTIxNXg1MmZIZTFDRTZGRjd5Q09TdjJYblkwVlNiWXFYcE1yaHVrVmZqTVJUVEE4X1VwZjhTd1JxcjRNdmFrSmh2RCZmPTAxVlVLVzVWSlpLSjZVWkFGTkVORVlLN1pQUERCRDZKSVAmYz0lMkYmYT1Mb29wQXBwJnA9JTQwZmx1aWR4JTJGbG9vcC1wYWdlLWNvbnRhaW5lciZ4PSU3QiUyMnclMjIlM0ElMjJUMFJUVUh4a2EzTjFibVF1YzJoaGNtVndiMmx1ZEM1amIyMThZaUUxZURVeVpraGxNVU5GTmtaR04zbERUMU4yTWxodVdUQldVMkpaY1Zod1RYSm9kV3RXWm1wTlVsUlVRVGhmVlhCbU9GTjNVbkZ5TkUxMllXdEthSFpFZkRBeFZsVkxWelZXU1RJMVJsaFBNalkyUlZkQ1FqTTFRVmhKVTBkRFVVcFdXa1klM0QlMjIlMkMlMjJpJTIyJTNBJTIyNzRmNzM1ZmUtYzg4Ny00MjhhLWFkZmYtNTEyZTg2YmNmZmQzJTIyJTdE) \n",
    "(**Writing your own notebooks**) on loop for more details). Replace **some_string** with a name that makes sense for your notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp Spyogenes_parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "Include all the libraries which should be used in this module. You can also import modules from other notebooks; here, we have imported the functions in the core notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "# standard libs\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Common to template\n",
    "# add into settings.ini, requirements, package name is python-dotenv, for conda build ensure `conda config --add channels conda-forge`\n",
    "import dotenv  # for loading config from .env files, https://pypi.org/project/python-dotenv/\n",
    "import envyaml  # Allows to loads env vars into a yaml file, https://github.com/thesimj/envyaml\n",
    "import fastcore  # To add functionality related to nbdev development, https://github.com/fastai/fastcore/\n",
    "from fastcore import (\n",
    "    test,\n",
    ")\n",
    "from fastcore.script import (\n",
    "    call_parse,\n",
    ")  # for @call_parse, https://fastcore.fast.ai/script\n",
    "import json  # for nicely printing json and yaml\n",
    "\n",
    "#import functions from core module (optional, but most likely needed). \n",
    "from ssi_analysis_result_parsers import(\n",
    "    core,\n",
    "    blast_parser,\n",
    ")\n",
    "#from ssi_analysis_result_parsers.blast_parser import extract_presence_absence\n",
    "\n",
    "# Project specific libraries\n",
    "from pathlib import Path\n",
    "import pandas\n",
    "import numpy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block should never be exported. It is to have python running in the project (and not the nbs) dir, and to initiate the package using pip.\n",
    "os.chdir(core.PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Add your code here below. If your notebook will be used as a console-script, you need to add a \"cli\"-function, at the end (see [Coding in NBdev](https://dksund.sharepoint.com/:fl:/g/contentstorage/CSP_7c761ee7-b577-4e08-8517-bc82392bf65e/ETlSfUyArSNJhX8veMI_JQ8By1aXGHzDJkhotpfpXx4mmw?e=037EwH&nav=cz0lMkZjb250ZW50c3RvcmFnZSUyRkNTUF83Yzc2MWVlNy1iNTc3LTRlMDgtODUxNy1iYzgyMzkyYmY2NWUmZD1iJTIxNXg1MmZIZTFDRTZGRjd5Q09TdjJYblkwVlNiWXFYcE1yaHVrVmZqTVJUVEE4X1VwZjhTd1JxcjRNdmFrSmh2RCZmPTAxVlVLVzVWSlpLSjZVWkFGTkVORVlLN1pQUERCRDZKSVAmYz0lMkYmYT1Mb29wQXBwJnA9JTQwZmx1aWR4JTJGbG9vcC1wYWdlLWNvbnRhaW5lciZ4PSU3QiUyMnclMjIlM0ElMjJUMFJUVUh4a2EzTjFibVF1YzJoaGNtVndiMmx1ZEM1amIyMThZaUUxZURVeVpraGxNVU5GTmtaR04zbERUMU4yTWxodVdUQldVMkpaY1Zod1RYSm9kV3RXWm1wTlVsUlVRVGhmVlhCbU9GTjNVbkZ5TkUxMllXdEthSFpFZkRBeFZsVkxWelZXU1RJMVJsaFBNalkyUlZkQ1FqTTFRVmhKVTBkRFVVcFdXa1klM0QlMjIlMkMlMjJpJTIyJTNBJTIyNzRmNzM1ZmUtYzg4Ny00MjhhLWFkZmYtNTEyZTg2YmNmZmQzJTIyJTdE) \n",
    "(**Code execution** and **Input, output and options**) on loop for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def extract_emm_type(emm_blast_tsv: Path):\n",
    "    \"\"\"   \n",
    "    with open(emm_cluster_file) as f:\n",
    "        emm_clusters = {}\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\").split()\n",
    "            for ele in line:\n",
    "                emm_clusters[ele] = line[0]\n",
    "             \n",
    "    try:\n",
    "        mga_blast_df = pandas.read_csv(mga_blast_tsv, sep='\\t', header = None)\n",
    "        mga_blast_df.columns = \"qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore\".split(' ')\n",
    "        mga_blast_df.sort_values(by=['bitscore'], ascending= False).iloc[0]\n",
    "        mga_pos = mga_blast_df.iloc[0]['sstart']\n",
    "\n",
    "    except pandas.errors.EmptyDataError:\n",
    "        print(f\"No mga matches found in assembly\")\n",
    "    \"\"\"\n",
    "\n",
    "    emm_types_in_emm_plus_mrp_operons = [] ### to update\n",
    "    mrp_types_in_emm_plus_mrp_operons = [\"134\",\"156\",\"159\",\"164\",\"174\",\"205\"] ### to update\n",
    "    emm_blast_tsv = Path(emm_blast_tsv)\n",
    "    emm_typing_results = {\"EMM_type\":\"-\",\"ENN_type\":\"-\",\"MRP_type\":\"-\"}\n",
    "    if not emm_blast_tsv.exists():\n",
    "        emm_typing_results[\"emm_typing_notes\"] = \"No blast output found for EMM genes\"\n",
    "        return(emm_typing_results)\n",
    "    else:\n",
    "        try:\n",
    "            blast_df = pandas.read_csv(emm_blast_tsv, sep='\\t', header = None)\n",
    "        except pandas.errors.EmptyDataError:\n",
    "            emm_typing_results[\"emm_typing_notes\"] = \"Empty blast output, no EMM genes detected\"\n",
    "            return(emm_typing_results)\n",
    "    notes = []\n",
    "    blast_df.columns = \"qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore\".split(' ')\n",
    "    blast_df[\"plen\"] = blast_df[\"length\"]/blast_df[\"qlen\"]*100\n",
    "    blast_df[\"extended_sstart\"] = numpy.where(blast_df[\"sstart\"]<blast_df[\"send\"], round((blast_df[\"sstart\"]-blast_df[\"qstart\"]+1)/100), round((blast_df[\"send\"]-blast_df[\"qstart\"]+1)/100))\n",
    "    blast_df = blast_df.query(\"bitscore > 200\")\n",
    "    blast_df_unique = blast_df.sort_values(by=['bitscore'], ascending= False).groupby(\"extended_sstart\").first()\n",
    "    \n",
    "    if blast_df_unique.shape[0] == 0:\n",
    "        notes.append(\"No blast hits found for EMM genes\")\n",
    "    elif len(set(blast_df_unique[\"sseqid\"])) == 1:\n",
    "        if blast_df_unique.shape[0] == 1:\n",
    "            emm_typing_results[\"EMM_type\"] = \"EMM\"+blast_df_unique.iloc[0][\"qseqid\"][3:]\n",
    "            if blast_df_unique.iloc[0][\"length\"] < blast_df_unique.iloc[0][\"qlen\"] or blast_df_unique.iloc[0][\"pident\"] < 100:\n",
    "                emm_typing_results[\"EMM_type\"] += \"*\"\n",
    "                notes.append(f\"EMM{blast_df_unique.iloc[0]['qseqid'][3:]} with {round(blast_df_unique.iloc[0]['pident'],2)} and length {blast_df_unique.iloc[0]['length']}/{blast_df_unique.iloc[0]['qlen']}\")\n",
    "        else:\n",
    "            if blast_df_unique.iloc[0][\"sstart\"] < blast_df_unique.iloc[0][\"send\"]:\n",
    "                blast_df_unique = blast_df_unique.sort_values(by=['sstart'], ascending=True)\n",
    "            else:\n",
    "                blast_df_unique = blast_df_unique.sort_values(by=['sstart'], ascending=False)\n",
    "            if blast_df_unique.shape[0] == 2:\n",
    "                emm_typing_results[\"EMM_type\"] = \"EMM\"+blast_df_unique.iloc[0][\"qseqid\"][3:]\n",
    "                if blast_df_unique.iloc[0][\"length\"] < blast_df_unique.iloc[0][\"qlen\"] or blast_df_unique.iloc[0][\"pident\"] < 100:\n",
    "                    emm_typing_results[\"EMM_type\"] += \"*\"\n",
    "                    notes.append(f\"EMM{blast_df_unique.iloc[0]['qseqid'][3:]} with pident {round(blast_df_unique.iloc[0]['pident'],2)} and length {blast_df_unique.iloc[0]['length']}/{blast_df_unique.iloc[0]['qlen']}\")\n",
    "                \n",
    "                emm_typing_results[\"ENN_type\"] = \"EMM\"+blast_df_unique.iloc[1][\"qseqid\"][3:]\n",
    "                if blast_df_unique.iloc[1][\"length\"] < blast_df_unique.iloc[1][\"qlen\"] or blast_df_unique.iloc[1][\"pident\"] < 100:\n",
    "                    emm_typing_results[\"ENN_type\"] += \"*\"\n",
    "                    notes.append(f\"ENN{blast_df_unique.iloc[1]['qseqid'][3:]} with pident {round(blast_df_unique.iloc[1]['pident'],2)} and length {blast_df_unique.iloc[1]['length']}/{blast_df_unique.iloc[1]['qlen']}\")\n",
    "                emm_maintype = blast_df_unique.iloc[0][\"qseqid\"][3:].split('.')[0]\n",
    "                mrp_maintype = blast_df_unique.iloc[1][\"qseqid\"][3:].split('.')[0]\n",
    "                if mrp_maintype in emm_types_in_emm_plus_mrp_operons or emm_maintype in mrp_types_in_emm_plus_mrp_operons:\n",
    "                    emm_typing_results[\"MRP_type\"] = \"EMM\"+emm_typing_results[\"EMM_type\"][3:]\n",
    "                    emm_typing_results[\"EMM_type\"] = \"EMM\"+emm_typing_results[\"ENN_type\"][3:]\n",
    "                    emm_typing_results[\"ENN_type\"] = '-'\n",
    "                    notes.append(f\"EMM redesignated due to known MRP+EMM operon\")\n",
    "\n",
    "\n",
    "            elif blast_df_unique.shape[0] == 3:\n",
    "                emm_typing_results[\"MRP_type\"] = \"EMM\"+blast_df_unique.iloc[0][\"qseqid\"][3:]\n",
    "                if blast_df_unique.iloc[0][\"length\"] < blast_df_unique.iloc[0]['qlen'] or blast_df_unique.iloc[0][\"pident\"] < 100:\n",
    "                    emm_typing_results[\"MRP_type\"] += \"*\"\n",
    "                    notes.append(f\"MRP{blast_df_unique.iloc[0]['qseqid'][3:]} with pident {round(blast_df_unique.iloc[0]['pident'],2)} and length {blast_df_unique.iloc[0]['length']}/{blast_df_unique.iloc[0]['qlen']}\")\n",
    "                \n",
    "                emm_typing_results[\"EMM_type\"] = \"EMM\"+blast_df_unique.iloc[1][\"qseqid\"][3:]\n",
    "                if blast_df_unique.iloc[1][\"length\"] < blast_df_unique.iloc[1]['qlen'] or blast_df_unique.iloc[1][\"pident\"] < 100:\n",
    "                    emm_typing_results[\"EMM_type\"] += \"*\"\n",
    "                    notes.append(f\"EMM{blast_df_unique.iloc[1]['qseqid'][3:]} with pident {round(blast_df_unique.iloc[1]['pident'],2)} and length {blast_df_unique.iloc[1]['length']}/{blast_df_unique.iloc[1]['qlen']}\")\n",
    "\n",
    "                emm_typing_results[\"ENN_type\"] = \"EMM\"+blast_df_unique.iloc[2][\"qseqid\"][3:]\n",
    "                if blast_df_unique.iloc[2][\"length\"] < blast_df_unique.iloc[2]['qlen'] or blast_df_unique.iloc[2][\"pident\"] < 100:\n",
    "                    emm_typing_results[\"ENN_type\"] += \"*\"\n",
    "                    notes.append(f\"ENN{blast_df_unique.iloc[2]['qseqid'][3:]} with pident {round(blast_df_unique.iloc[2]['pident'],2)} and length {blast_df_unique.iloc[2]['length']}/{blast_df_unique.iloc[2]['qlen']}\")\n",
    "    else:\n",
    "        emm_genes = []\n",
    "        for index, row in blast_df_unique.iterrows():\n",
    "            if row[\"length\"] < row[\"qlen\"] or row[\"pident\"] < 100:\n",
    "                emm_genes.append(row[\"qseqid\"][3:]+\"*\")\n",
    "            else:\n",
    "                emm_genes.append(row[\"qseqid\"][3:])\n",
    "        notes.append(\"EMM and EMM-like genes found on multiple contigs. Alleles found: \"+\"/\".join(emm_genes))\n",
    "        \n",
    "\n",
    "    emm_typing_results[\"emm_typing_notes\"] = \", \".join(notes)\n",
    "    return emm_typing_results\n",
    "\n",
    "\n",
    "class SpyogenesResults(core.PipelineResults):\n",
    "\n",
    "    @classmethod\n",
    "    def from_tool_paths(cls, emm_blast_tsv: Path, sample_name = None):\n",
    "        \"\"\"\n",
    "        Alternative constructor for initializing results for single sample,\n",
    "        Initializes SpyogenesResults instance provided paths to outputs from tools (legionella sbt and lag1 presence blast)\n",
    "        \"\"\"\n",
    "        gas_results = cls.summary(emm_blast_tsv=emm_blast_tsv)\n",
    "        return cls( {sample_name: gas_results})\n",
    "    \n",
    "    @classmethod\n",
    "    def from_tool_paths_dict(cls, file_paths: dict):\n",
    "        \"\"\"\n",
    "        Alternative constructor for initializing results for multiple samples,\n",
    "        Initializes SpyogenesResults instance by providing a dictionary of paths to outputs from tools (legionella sbt and lag1 presence blast)\n",
    "        \"\"\"\n",
    "        results_dict = {}\n",
    "        for sample_name, path_dict in file_paths.items():\n",
    "            gas_results = cls.summary(emm_blast_tsv=Path(path_dict[\"emm_results\"]))\n",
    "            results_dict[sample_name] = gas_results\n",
    "        return cls(results_dict)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_tool_paths_dataframe(cls, file_paths_df: pandas.DataFrame):\n",
    "        \"\"\"\n",
    "        Alternative constructor for initializing results for multiple samples,\n",
    "        Initializes SpyogenesResults instance by providing a DataFrame of paths to outputs from tools (legionella sbt and lag1 presence blast)\n",
    "        \"\"\"\n",
    "        file_paths = file_paths_df.to_dict(orient=\"index\")\n",
    "        results_dict = {}\n",
    "        for sample_name, path_dict in file_paths.items():\n",
    "            results = cls.summary(emm_blast_tsv=Path(path_dict[\"emm_results\"]))\n",
    "            results_dict[sample_name] = results\n",
    "        return cls(results_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def from_tool_paths_tsv(cls, tool_paths_tsv: Path):\n",
    "        \"\"\"\n",
    "        Alternative constructor for initializing results for multiple samples,\n",
    "        Initializes SpyogenesResults instance by providing a tsv-file with paths to outputs from tools (legionella sbt and lag1 presence blast)\n",
    "        \"\"\"\n",
    "        file_paths_df = pandas.read_csv(tool_paths_tsv, sep='\\t')\n",
    "        file_paths_df.set_index(\"sample_name\", inplace=True, drop=True)\n",
    "        return cls.from_tool_paths_dataframe(file_paths_df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def summary(emm_blast_tsv: Path) -> dict:\n",
    "        emm_results = extract_emm_type(emm_blast_tsv=emm_blast_tsv)\n",
    "        results_dict = emm_results\n",
    "        #results_dict = core.update_results_dict(sbt_results_dict, lag1_blast_dict, old_duplicate_key_prefix=\"SBT: \")\n",
    "        if results_dict is None:\n",
    "            return {}\n",
    "        return results_dict\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return(f\"< Spyogenes analysis results object. {len(self.results_df)} samples with {len(self.results_df.columns)} result variables > \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "emm_typing_dict = extract_emm_type(emm_blast_tsv=\"test_input/Spyogenes/emm_typing/test1.emm.blast.tsv\")\n",
    "\n",
    "assert(emm_typing_dict[\"EMM_type\"] == \"EMM87.0\")\n",
    "assert(emm_typing_dict[\"ENN_type\"] == \"EMM159.0*\")\n",
    "assert(emm_typing_dict[\"MRP_type\"] == \"-\")\n",
    "assert(emm_typing_dict[\"emm_typing_notes\"] == \"ENN159.0 with pident 97.78 and length 180/180\")\n",
    "\n",
    "\n",
    "\n",
    "Spyogenes_results = SpyogenesResults.from_tool_paths(emm_blast_tsv=\"test_input/Spyogenes/emm_typing/test2.emm.blast.tsv\", sample_name=\"test_sample\")\n",
    "\n",
    "assert(list(Spyogenes_results.results_dict.keys())[0] == \"test_sample\")\n",
    "\n",
    "Spyogenes_results = SpyogenesResults.from_tool_paths_dict(file_paths=  {\"sample_3\": {\"emm_results\": \"test_input/Spyogenes/emm_typing/test3.emm.blast.tsv\"},\n",
    "                                                                        \"sample_4\": {\"emm_results\": \"test_input/Spyogenes/emm_typing/test4.emm.blast.tsv\"}})\n",
    "\n",
    "assert(Spyogenes_results.results_dict[\"sample_3\"][\"EMM_type\"] == \"EMM77.0\")\n",
    "assert(Spyogenes_results.results_dict[\"sample_3\"][\"ENN_type\"] == \"EMM159.0\")\n",
    "assert(Spyogenes_results.results_dict[\"sample_3\"][\"MRP_type\"] == \"EMM141.3*\")\n",
    "assert(Spyogenes_results.results_dict[\"sample_3\"][\"emm_typing_notes\"] == \"MRP141.3 with pident 97.22 and length 180/180\")\n",
    "\n",
    "\n",
    "\n",
    "assert(Spyogenes_results.results_df[\"EMM_type\"][\"sample_4\"] == \"EMM13.0\")\n",
    "assert(Spyogenes_results.results_df[\"ENN_type\"][\"sample_4\"] == \"EMM203.4*\")\n",
    "assert(Spyogenes_results.results_df[\"MRP_type\"][\"sample_4\"] == \"EMM141.4\")\n",
    "assert(Spyogenes_results.results_df[\"emm_typing_notes\"][\"sample_4\"] == \"ENN203.4 with pident 97.22 and length 180/180\")\n",
    "\n",
    "\n",
    "Spyogenes_results = SpyogenesResults.from_tool_paths_tsv(tool_paths_tsv=\"test_input/Spyogenes/batch_parser_file_paths.tsv\")\n",
    "\n",
    "\n",
    "assert(Spyogenes_results.results_dict[\"sample_5\"][\"EMM_type\"] == \"EMM81.0\")\n",
    "assert(Spyogenes_results.results_dict[\"sample_5\"][\"ENN_type\"] == \"-\")\n",
    "assert(Spyogenes_results.results_dict[\"sample_5\"][\"MRP_type\"] == \"EMM156.4*\")\n",
    "assert(Spyogenes_results.results_dict[\"sample_5\"][\"emm_typing_notes\"] == \"EMM156.4 with pident 99.44 and length 180/180, EMM redesignated due to known MRP+EMM operon\")\n",
    "\n",
    "\n",
    "assert(Spyogenes_results.results_df[\"EMM_type\"][\"sample_7\"] == \"-\")\n",
    "assert(Spyogenes_results.results_df[\"ENN_type\"][\"sample_7\"] == \"-\")\n",
    "assert(Spyogenes_results.results_df[\"MRP_type\"][\"sample_7\"] == \"-\")\n",
    "assert(Spyogenes_results.results_df[\"emm_typing_notes\"][\"sample_7\"] == \"EMM and EMM-like genes found on multiple contigs. Alleles found: 203.4*/28.0\")\n",
    "\n",
    "assert(Spyogenes_results.results_df[\"EMM_type\"][\"sample_empty\"] == \"-\")\n",
    "assert(Spyogenes_results.results_df[\"ENN_type\"][\"sample_empty\"] == \"-\")\n",
    "assert(Spyogenes_results.results_df[\"MRP_type\"][\"sample_empty\"] == \"-\")\n",
    "assert(Spyogenes_results.results_df[\"emm_typing_notes\"][\"sample_empty\"] == \"Empty blast output, no EMM genes detected\")\n",
    "\n",
    "\n",
    "assert(Spyogenes_results.results_df[\"EMM_type\"][\"sample_nonexist\"] == \"-\")\n",
    "assert(Spyogenes_results.results_df[\"ENN_type\"][\"sample_nonexist\"] == \"-\")\n",
    "assert(Spyogenes_results.results_df[\"MRP_type\"][\"sample_nonexist\"] == \"-\")\n",
    "assert(Spyogenes_results.results_df[\"emm_typing_notes\"][\"sample_nonexist\"] == \"No blast output found for EMM genes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def Spyogenes_parser(\n",
    "    emm_blast_tsv: Path = None,  # Blast output from blasting EMM and emm-like genes\n",
    "    output_file: Path = None,  # Path to output tsv\n",
    "    sample_name: str = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    results = SpyogenesResults.from_tool_paths(emm_blast_tsv=emm_blast_tsv,\n",
    "                                                           sample_name=sample_name)\n",
    "    results.write_tsv(output_file=output_file)\n",
    "\n",
    "@call_parse\n",
    "def Spyogenes_batch_parser(\n",
    "    file_path_tsv: Path = None,  # Path to tsv containing file paths to the outputs from tools to be parsed. Must contain headers \"sample_name\", \"sbt_results\", and \"lag1_blast_results\"\n",
    "    output_file: Path = None,  # Path to output tsv\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    results = SpyogenesResults.from_tool_paths_tsv(tool_paths_tsv=file_path_tsv)\n",
    "    results.write_tsv(output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directive for ensuring that the code in your notebook get executed as a script\n",
    "\n",
    "The code-block here below is required to ensure that the code in the notebook is also transferred to the module (script), otherwise it will just be a notebook. See [Coding in NBdev](https://dksund.sharepoint.com/:fl:/g/contentstorage/CSP_7c761ee7-b577-4e08-8517-bc82392bf65e/ETlSfUyArSNJhX8veMI_JQ8By1aXGHzDJkhotpfpXx4mmw?e=037EwH&nav=cz0lMkZjb250ZW50c3RvcmFnZSUyRkNTUF83Yzc2MWVlNy1iNTc3LTRlMDgtODUxNy1iYzgyMzkyYmY2NWUmZD1iJTIxNXg1MmZIZTFDRTZGRjd5Q09TdjJYblkwVlNiWXFYcE1yaHVrVmZqTVJUVEE4X1VwZjhTd1JxcjRNdmFrSmh2RCZmPTAxVlVLVzVWSlpLSjZVWkFGTkVORVlLN1pQUERCRDZKSVAmYz0lMkYmYT1Mb29wQXBwJnA9JTQwZmx1aWR4JTJGbG9vcC1wYWdlLWNvbnRhaW5lciZ4PSU3QiUyMnclMjIlM0ElMjJUMFJUVUh4a2EzTjFibVF1YzJoaGNtVndiMmx1ZEM1amIyMThZaUUxZURVeVpraGxNVU5GTmtaR04zbERUMU4yTWxodVdUQldVMkpaY1Zod1RYSm9kV3RXWm1wTlVsUlVRVGhmVlhCbU9GTjNVbkZ5TkUxMllXdEthSFpFZkRBeFZsVkxWelZXU1RJMVJsaFBNalkyUlZkQ1FqTTFRVmhKVTBkRFVVcFdXa1klM0QlMjIlMkMlMjJpJTIyJTNBJTIyNzRmNzM1ZmUtYzg4Ny00MjhhLWFkZmYtNTEyZTg2YmNmZmQzJTIyJTdE) \n",
    "(**Writing your own notebooks**) on loop for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# This is included at the end to ensure when you run through your notebook the code is also transferred to the module and isn't just a notebook\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
